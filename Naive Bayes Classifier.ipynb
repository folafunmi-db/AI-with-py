{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "Naïve Bayes is a technique used to build classifiers using Bayes theorem. Bayes theorem describes the probability of an event occurring based on different conditions that are related to this event. We build a Naïve Bayes classifier by assigning class labels to problem instances. These problem instances are represented as vectors of feature values. The assumption here is that the value of any given feature is independent of the value of any other feature. This is called the independence assumption, which is the naïve part of a Naïve Bayes classifier. \n",
    "\n",
    "Given the class variable, we can just see how a given feature affects, it regardless of its affect on other features. For example, an animal may be considered a cheetah if it is spotted, has four legs, has a tail, and runs at about 70 MPH. A Naïve Bayes classifier considers that each of these features contributes independently to the outcome. The outcome refers to the probability that this animal is a cheetah. We don't concern ourselves with the correlations that may exist between skin patterns, number of legs, presence of a tail, and movement speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classifier(classifier, X, y):\n",
    "    # Define the mininum and maximum values for X and y that will\n",
    "    # be used in the mesh gridd\n",
    "    min_x, max_x = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n",
    "    min_y, max_y = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n",
    "    \n",
    "    # Define the step size to use in plotting the mesh grid\n",
    "    mesh_step_size = 0.01\n",
    "    \n",
    "    # Define teh mesh grid of the X and y values\n",
    "    x_vals, y_vals = np.meshgrid(np.arange(min_x, max_x, mesh_step_size), \\\n",
    "    np.arange(min_y, max_y, mesh_step_size))\n",
    "    \n",
    "    # Run the classifier\n",
    "    output = classifier.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n",
    "    \n",
    "    # Reshape the output array\n",
    "    output = output.reshape(x_vals.shape)\n",
    "    \n",
    "    # Create a plot\n",
    "    plt.figure()\n",
    "    \n",
    "    # Choose a colour scheme for the plot\n",
    "    plt.pcolormesh(x_vals, y_vals, output, cmap=plt.cm.gray)\n",
    "    \n",
    "    # Overlay the trainin points on the plot\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=75, edgecolors='black',\\\n",
    "               linewidth=1, cmap=plt.cm.Paired)\n",
    "    \n",
    "    # Specify the boundaries of the plot\n",
    "    plt.xlim(x_vals.min(), x_vals.max())\n",
    "    plt.ylim(y_vals.min(), y_vals.max())\n",
    "    \n",
    "    plt.xticks((np.arange(int(X[:, 0].min() - 1), int(X[:, 0].max() + 1), 1.0)))\n",
    "    plt.yticks((np.arange(int(X[:, 1].min() - 1), int(X[:, 1].max() + 1), 1.0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
